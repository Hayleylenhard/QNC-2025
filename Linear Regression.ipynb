{"cells":[{"cell_type":"markdown","metadata":{"id":"pKIiY6p3GRFq"},"source":["# Definitions"]},{"cell_type":"markdown","metadata":{"id":"x7VmLUr5GTNw"},"source":["Linear regression is a common way to measure associations between a dependent, measured random variable ($Y$) and a set of independent predictors ($X$).\n","\n","**Simple linear regression** uses just one predictor:\n","\n","$\\quad y_i=\\beta_0+\\beta_1x_1+\\epsilon_i$\n","\n","where $\\beta_0$ and $\\beta_1$ are the \"beta weights\" describing the y-intercept and slope, respectively, of the line describing the relationship between $X$ and $Y$, and $\\epsilon_i$ tells us that the relationship is noisy (more specifically, that the measured linear relationship is corrupted by additive Gaussian noise).\n","\n","**Multiple linear regression** uses multiple predictors:\n","\n","$\\quad y_i=\\beta_0+\\beta_1x_1+\\beta_2x_2\\:+ ... +\\:\\epsilon_i$\n","\n","**General linear models**, which we will not consider further here, are even more general and allow the value of $Y$ to be a vector and not just a scalar on each trial.\n","\n","We typically use linear regression to do any or all of the following:\n","\n","1\\. Infer if a measured variable $Y$ is associated with another variable (predictor) of set of variables (predictors) $X$. This is a hypothesis test. For example, to test if there is a positive relationship between two variables: as you are given more ice cream to eat ($X$ variable), does your body weight ($Y$ variable) change (increase or decrease)? A *p*-value can be assigned to this association; more below on this.\n","\n","2\\. What is the strength of the association? How tightly are these two variables associated? A positive beta weight implies a positive relationship (both go up/down at the same time), whereas a negative value implies a negative relationship (they go in opposite directions).\n","\n","It is critical to remember that unlike a correlation coefficient, whose magnitude always varies between -1 and 1 independently of the magnitude of the quantities being correlated, in linear regression the magnitude of any given beta weight depends critically on the units and magnitudes of the quantities used in the regression. Therefore, comparing beta weights from two different regressions (or even from different terms in the same multiple linear regression equation, if the predictors are not independent!) is not always straightforward, as discussed [here](http://www.glmj.org/archives/articles/Ziglari_v43n2.pdf).\n","\n","One way around this problem is to use standardized linear regression, in which the same linear model is used but applied to standardized variables. That is, each of the independent and dependent variables are converted in units of z-score: subtract the sample mean and divide by the sample standard deviation.\n","\n","3\\. The equation that describes the relationship can be used to predict unknown values. That is, once the values of the betas in the above equations are known, they can be used with new values of the independent variables to predict the value of the dependent variable under those conditions.\n","\n","There are some assumptions in order to use linear regression. For example, for any value of $X$ there exists a normal distribution of $Y$ values, and the relationship between $X$ and $Y$ is linear. Make sure that you always look at your data first (e.g., in scatterplots) to get a first-pass impression if these assumptions are reasonable."]},{"cell_type":"markdown","metadata":{"id":"IyCyVIIDH0rb"},"source":["# Calculating the slope of the regression line and its intercept"]},{"cell_type":"markdown","metadata":{"id":"sYDtxlCZH5BC"},"source":["The **slope** $b=\\frac{\\sum{X_iY_i-\\frac{\\sum{X_i}\\sum{Y_i}}{n}}}{\\sum{X^2_i-\\frac{(\\sum{X_i})^2}{n}}}$, where *i* is the individual values of $X$ an $Y$ and *n* is the number of observations.  This calculation is the best fit or best estimate of the slope, given your measurements. It is best in the sense that it tries to minimize the distance (in a least squares sense) between $Y_{pred}$ and all $Y_i$.\n","\n","The **intercept** $a=Y_{avg} - bX_{avg}$, where $Y_{avg}$ and $X_{avg}$ are the average values of $X$ and $Y$."]},{"cell_type":"markdown","metadata":{"id":"xDpz-ktIJGhf"},"source":["# Null hypotheses for linear regression\n"]},{"cell_type":"markdown","metadata":{"id":"c-qA27fbJM6k"},"source":["For the slope, this test can be conceptualized in two ways:\n","\n","1\\. $H_0: b=0$. Here an [analysis of variance](https://colab.research.google.com/drive/1wPjM-On_ktPzcV4xv5iP5nW8t0IPBWz1?usp=sharing) and an *F*-statistic ($df=n-2$) is used to support or refute the null hypothesis.\n","\n","2\\. (More general) $H_0:b=b_0$, where $b_0$ can be any value. Here, a 2-tailed [*t*-test](https://colab.research.google.com/drive/1M7xjaMwJUEyULPHfXc3tWG6-WVjCl-uQ?usp=sharing) can be used to test the null hypothesis. 1-tailed *t*-tests can also be used to test null hypotheses such as $H_0:b≤0$ or $H_0:b≥0$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6UC6Zg966XRY"},"source":["# Confidence intervals in linear regression"]},{"cell_type":"markdown","metadata":{"id":"EQC-5Cti7Vui"},"source":["Confidence intervals in linear regression are based on the standard errors of the estimates of the regression parameters and can be computed using the [*t* distribution](https://colab.research.google.com/drive/1Q_Du5NK71Rc1qu-byh8dy8Fs39uvR_4n?usp=sharing):\n","\n","For the slope, the $100(1-\\alpha)$ confidence interval is defined as:\n","\n","$\\quad b\\pm t_{\\alpha/2,n-2}\\times\\sqrt{\\frac{MSE}{\\sum(x_i-\\bar{x})^2}}$, where MSE is the mean squared error [defined in the ANOVA tutorial](https://colab.research.google.com/drive/1wPjM-On_ktPzcV4xv5iP5nW8t0IPBWz1?usp=sharing).\n","\n","For the intercept, the $100(1-\\alpha)$ confidence interval is defined as:\n","\n","$\\quad a\\pm t_{\\alpha/2,n-2}\\times\\sqrt{\\frac{MSE}{n}}$."]},{"cell_type":"markdown","metadata":{"id":"EI84_xI77WZH"},"source":["# Sample size and power"]},{"cell_type":"markdown","metadata":{"id":"4dGvHbgR7X4G"},"source":["Like with all experiments, it is always a good idea to get a sense of how much data you will need to reject $H_0$. See [here](https://colab.research.google.com/drive/1wTKRgKK5eDUya7FZRHeu1RaoY7kuhiGi?usp=sharing) for how to perform a power analysis using a *t*-test."]},{"cell_type":"markdown","metadata":{"id":"nvmWeSSHSF95"},"source":["# Neuroscience Examples"]},{"cell_type":"markdown","metadata":{"id":"vAVidQDc8wIT"},"source":["## Example 1: London Taxi Drivers"]},{"cell_type":"markdown","metadata":{"id":"hsGDKKtU82yq"},"source":["Perhaps one of the more famous early studies using MRI and fMRI to relate behavior to underlying neuroanatomy are the studies that looked at relationships between experience as a taxi-cab drivers in London and hippocampus size. In [the first study](https://www.pnas.org/content/97/8/4398), they found a correlation between hippocampal size (in the posterior region]) and time as a taxi-cab driver. In London, taxi drivers have to take a test showing their knowledge of the streets of London. Here is one of their key findings:\n","\n","![](https://drive.google.com/uc?export=view&id=1ozXY-A4yvF8NNgY6jwD8ZuiIjsjQwYjZ)\n","\n","**Legend**: The volume of gray matter in the right hippocampus was found to correlate significantly with the amount of time spent learning to be and practicing as a licensed London taxi driver, positively in the right posterior hippocampus.\n","\n","Without getting into details of the science itself, do you think a liner regression is a proper way to analyze this data set? Are the data linear? Why or why not? If not, how does that weaken any interpretation of the data?\n","\n","In a [follow-up study](https://onlinelibrary.wiley.com/doi/epdf/10.1002/hipo.20233), they compared taxi cab drivers with bus drivers and found a correlation between years of driving experience and hippocampal size for the taxi cab drivers but not for the bus drivers. They argued that the basis for these results is the observation that bus drivers drive the same route everyday, whereas taxi cab drivers have to know the entire city and be flexible in how they navigate the city. Can one make any sort of inference about casualty from this finding? What are the weaknesses and strengths of such a study and set of findings? \n","\n"]},{"cell_type":"markdown","metadata":{"id":"j0jyX3MQ_J5V"},"source":["## Example 2: Musical Brains"]},{"cell_type":"markdown","metadata":{"id":"qgiPYCno_QTT"},"source":["Here is [another example](http://www.brainmusic.org/EducationalActivities/Pantev_musicians1998.pdf), in which a group of scientists attempt to relate behavioral function with changes in underlying neuroanatomy. This is not bad, mind you, but fraught with complexities. In this study, they studied how musical information was represented in the brains of musicians with perfect (absolute) pitch (i.e.,  can recognize a note or reproduce a note without any reference note) or relative pitch (need a reference note) and a control group. Here is their first figure: \n","\n","![](https://drive.google.com/uc?export=view&id=18C36zvXPjZ7_wk04IMD9OshVQTrAW1nB)\n","\n","In panel b, they plot the correlation between frequency and region of brain activation or each of the 3 groups. This correlation is done for pure tones (panel a, left; like those produced by a tuning fork) and for piano notes (panel a, right). What do you think of their *n*? How strong of a claim do you think they can make with their regression based on their *n*? What would their confidence intervals look like? In each plot, there are 2 regression lines. In theory, can you think of a way to  decide if the two regression lines are the same or not? What would be your $H_0$? \n","\n","Here is their second figure:\n","\n","![](https://drive.google.com/uc?export=view&id=1QW773k_M3U_mWk1aMrtvZ1hULpwPARW0)\n","\n","Panel b is like the taxi-cab paper in which they correlate musical experience (age in which they started playing an instrument) with a measure of brain activation (the higher the mean dipole moment, the more the neurons are nominally activated). Ignoring the two types of musicians, what is the take-home message from the graph? Is it ok to treat both sets of musicians as one group? What assumptions would you have to make? What tests do you think you should do(qualitatively speaking)? Do you think you should report (if this was your paper] the regression between moment and the two groups separately? "]},{"cell_type":"markdown","metadata":{"id":"RzC55KsbVrvw"},"source":["# Exercises"]},{"cell_type":"markdown","metadata":{"id":"bki88QRVSaj8"},"source":["Using the following data set to investigate the relationship between Age and Wing Length, and use the data to generate answers to the prompts below:\n","\n","Age\t| Wing Length\n","-- | --\n","3\t| 1.4\n","4\t| 1.5\n","5\t| 2.2\n","6\t| 2.4\n","7\t| 3.1\n","8\t| 3.2\n","9\t| 3.2\n","11\t| 3.9\n","12\t| 4.1\n","14\t| 4.7\n","15\t| 4.5\n","16\t| 5.2\n","17 | 5.0\n","\n","Answers to the exercises below will be found [here](https://github.com/PennNGG/Quantitative-Neuroscience/tree/master/Answers%20to%20Exercises/Python) after the due date."]},{"cell_type":"markdown","metadata":{"id":"zIfgeegzTlJl"},"source":["##### 1\\. Plot the relationship between Age and Wing Length."]},{"cell_type":"markdown","metadata":{"id":"v4ihoOv0TzTk"},"source":["##### 2\\. Calculate and plot the regression line."]},{"cell_type":"markdown","metadata":{"id":"XqoZSFdZU5zg"},"source":["##### 3\\. Can you reject $H_0:b=0$?"]},{"cell_type":"markdown","metadata":{"id":"sU0UjR6uVMl2"},"source":["##### 4\\. Calculate and plot the confidence intervals on the slope of the regression."]},{"cell_type":"markdown","metadata":{"id":"XaEBSR97qTEP"},"source":["##### 4\\. Calculate $r^2$ (the coefficient of determination)"]},{"cell_type":"markdown","metadata":{"id":"mywqPzz3Vg0I"},"source":["##### 6\\. Calculate Pearson's *r*."]},{"cell_type":"markdown","metadata":{"id":"qYW4bBQ7Vxa-"},"source":["##### 7\\. Add some noise to the data and see how the regression changes."]},{"cell_type":"markdown","metadata":{"id":"UqoNXyuxP-go"},"source":["# Additional Resources\n"]},{"cell_type":"markdown","metadata":{"id":"clnBO4FU28El"},"source":["- Differences between correlation and regression are discussed [here](https://www.bmj.com/about-bmj/resources-readers/publications/statistics-square-one/11-correlation-and-regression) and [here](http://www.biostathandbook.com/linearregression.html).\n","\n","- Fun applets are [here](https://www.desmos.com/calculator/jwquvmikhr) and [here](http://www.shodor.org/interactivate/activities/Regression/).\n","\n","- Working with linear regression in [Matlab](https://www.mathworks.com/help/matlab/data_analysis/linear-regression.html), [R](https://www.tutorialspoint.com/r/r_linear_regression.htm), and [Python](https://realpython.com/linear-regression-in-python/)."]},{"cell_type":"markdown","metadata":{"id":"tteEm2Qlgbb3"},"source":["# Credits\n","\n","Copyright 2021 by Joshua I. Gold, University of Pennsylvania"]}],"metadata":{"colab":{"collapsed_sections":["IyCyVIIDH0rb","xDpz-ktIJGhf","6UC6Zg966XRY","EI84_xI77WZH","NUeNn8deHBch","zIfgeegzTlJl","v4ihoOv0TzTk","XqoZSFdZU5zg","sU0UjR6uVMl2","mywqPzz3Vg0I","qYW4bBQ7Vxa-"],"name":"Linear Regression","provenance":[{"file_id":"11kgk7FpLgbSlA4pjS4cCH1B5mYDUgj5b","timestamp":1626738154275},{"file_id":"1dvkIh9KgmzwwJ7phFBxkHmS3nFTKN_yo","timestamp":1626368223618},{"file_id":"1AmfvDhhfviRQFvONiUVUba_6hof8RDp6","timestamp":1626367834690},{"file_id":"1wTKRgKK5eDUya7FZRHeu1RaoY7kuhiGi","timestamp":1626364730636},{"file_id":"1rdJMusMZDTaM9OGsyt27tCVkSasmRj2O","timestamp":1626357708093},{"file_id":"1HW0L_d5Wpod3jbnY3iG7mLhMG6yWHvF2","timestamp":1626350171621},{"file_id":"1-KxH3FCq5rDyyO33HXxewIv-kKldkINi","timestamp":1626290714843},{"file_id":"14S2ca44h8TKC1hFXjk5ktwBYpGU6R5S-","timestamp":1624411796822}]},"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.6"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
